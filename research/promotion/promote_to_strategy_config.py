#!/usr/bin/env python3
from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Any


def _yaml_scalar(v: Any) -> str:
    if isinstance(v, bool):
        return "true" if v else "false"
    if v is None:
        return "null"
    if isinstance(v, (int, float)):
        return str(v)
    # Quote strings safely for simple YAML (no fancy multiline)
    s = str(v)
    s = s.replace("\\", "\\\\").replace('"', '\\"')
    return f"\"{s}\""


def _yaml_map(d: dict[str, Any], indent: int = 0) -> str:
    pad = "  " * indent
    lines: list[str] = []
    for k in sorted(d.keys()):
        v = d[k]
        if isinstance(v, dict):
            lines.append(f"{pad}{k}:")
            lines.append(_yaml_map(v, indent=indent + 1))
        else:
            lines.append(f"{pad}{k}: {_yaml_scalar(v)}")
    return "\n".join(lines)


def main() -> int:
    parser = argparse.ArgumentParser(
        description="Promote an experiment run into a SAFE strategy config (disabled, eval-only)."
    )
    parser.add_argument("--experiment-id", required=True, help="Experiment id (folder under research/results)")
    parser.add_argument("--run-id", required=True, help="Run id (subfolder under research/results/<experiment_id>)")
    parser.add_argument("--strategy", required=True, help="Strategy config name (file name without .yaml)")
    parser.add_argument(
        "--results-root",
        default="research/results",
        help="Root directory for results (default: research/results).",
    )
    args = parser.parse_args()

    repo_root = Path(__file__).resolve().parents[2]
    run_dir = (repo_root / args.results_root / args.experiment_id / args.run_id).resolve()
    if not run_dir.exists():
        raise SystemExit(f"Run directory not found: {run_dir}")

    spec_path = run_dir / "spec.json"
    metrics_path = run_dir / "metrics.json"
    if not spec_path.exists() or not metrics_path.exists():
        raise SystemExit(f"Missing spec.json or metrics.json in: {run_dir}")

    spec_obj = json.loads(spec_path.read_text())
    metrics_obj = json.loads(metrics_path.read_text())

    spec = spec_obj.get("spec", {})
    exp_params = spec.get("parameters", {}) or {}
    metrics = metrics_obj.get("metrics", {}) or {}

    # Safe-by-default strategy config payload (no trading)
    cfg: dict[str, Any] = {
        "strategy": args.strategy,
        "enabled": False,
        "mode": "EVAL_ONLY",
        "requires_human_approval": True,
        "provenance": {
            "source_experiment": args.experiment_id,
            "source_run_id": args.run_id,
            "run_fingerprint": metrics_obj.get("run_fingerprint"),
            "build_id": metrics_obj.get("provenance", {}).get("build_id"),
            "git_sha": metrics_obj.get("provenance", {}).get("git_sha"),
        },
        # Parameters derived from experiment outputs:
        # - always include the experiment parameters
        # - optionally include selected metrics as calibration hints
        "parameters": {
            **exp_params,
            "calibration": {
                "sharpe_like": metrics.get("sharpe_like"),
                "hit_rate": metrics.get("hit_rate"),
                "max_drawdown": metrics.get("max_drawdown"),
            },
        },
        "safety": {
            "execution": "DISABLED",
            "note": "Generated by promotion tool; must be reviewed and explicitly enabled by a human.",
        },
    }

    out_dir = (repo_root / "configs" / "strategies").resolve()
    out_dir.mkdir(parents=True, exist_ok=True)
    out_path = out_dir / f"{args.strategy}.yaml"

    yaml_text = "# AUTO-GENERATED (safe-by-default). DO NOT enable without human review.\n"
    yaml_text += _yaml_map(cfg) + "\n"
    out_path.write_text(yaml_text)

    print(f"Wrote: {out_path}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())

